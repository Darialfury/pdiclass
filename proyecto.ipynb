{
 "metadata": {
  "name": "",
  "signature": "sha256:224b38d113892a2d03f1d212fb3df3c2e81d1940d795aa94e68fa0b516533342"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Proyect object recognition"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# set imports\n",
      "import cv2\n",
      "import cPickle as pickle\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "import os\n",
      "import scipy.io as sio\n",
      "import multiprocessing\n",
      "import random\n",
      "\n",
      "from multiprocessing import Pool\n",
      "from sklearn.cluster import KMeans\n",
      "from sklearn.cross_validation import train_test_split\n",
      "\n",
      "%matplotlib inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 62
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Functions to use\n",
      "\n",
      "# get all the lines of txt file and save them in a list\n",
      "def get_items_txt(txt_name):\n",
      "    lines_txt = open(txt_name, 'r')\n",
      "    list_text = []\n",
      "    for line in lines_txt:\n",
      "        newline = line.split('\\n')[0]\n",
      "        list_text.append(newline)\n",
      "    return list_text\n",
      "\n",
      "#save and object\n",
      "def save_object(obj, filename):\n",
      "    with open(filename, 'wb') as output:\n",
      "        pickle.dump(obj, output, -1)\n",
      "\n",
      "#load an object\n",
      "def load_object(filename):\n",
      "    with open(filename, 'rb') as input:\n",
      "        obj = pickle.load(input)\n",
      "    return obj\n",
      "\n",
      "def feature_quantization((file_mat, path, kmeans_object)):\n",
      "    folder = file_mat.split('/')[2]\n",
      "    name = file_mat.split('/')[3]\n",
      "    path_fq = path + folder + '/'\n",
      "    if not os.path.exists(path_fq):\n",
      "        os.makedirs(path_fq)\n",
      "    filename = path_fq + 'feature_' + name\n",
      "    var = sio.loadmat(file_mat)\n",
      "    array_to_stack = var['s_desc']\n",
      "    features = kmeans_object.predict(array_to_stack)\n",
      "    visual_words = np.histogram(features, bins = k_clusters, range=(0, k_clusters -1))\n",
      "    sio.savemat(filename, {'bow': visual_words})"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 63
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Features extraction"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This section compute sift descriptor with each image of the dataset. For each image exists a descriptor in a folder called \"images_sift\""
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Creating path for storing sift features\n",
      "newpath = r'./images_sift/'\n",
      "if not os.path.exists(newpath): os.makedirs(newpath)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 49
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# save in a list all folders of the dataset\n",
      "path_dataset = './101_ObjectCategories/'\n",
      "lstring_folders = os.listdir(path_dataset)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 50
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#feature extraction of all images\n",
      "\n",
      "#search for each folder of the dataset\n",
      "for folder in lstring_folders:\n",
      "    path_folder = path_dataset + folder + '/'\n",
      "    list_images = os.listdir(path_folder)\n",
      "    list_images.sort()\n",
      "    path2_folder = newpath + folder\n",
      "    if not os.path.exists(path2_folder):\n",
      "        os.makedirs(path2_folder)\n",
      "        \n",
      "        # search for each image of the folder and compute sift\n",
      "        for image in list_images:\n",
      "            path_image_load = path_folder + '/' + image\n",
      "            x = cv2.imread(path_image_load)\n",
      "            y = cv2.cvtColor(x,cv2.COLOR_BGR2GRAY)\n",
      "            sift = cv2.SIFT()\n",
      "            kp, des = sift.detectAndCompute(y,None)\n",
      "\n",
      "            # If descriptor are none, don't save it\n",
      "            if des is not None:\n",
      "                x2 = str(image.split('.',1)[0])\n",
      "                path2 = path2_folder + '/' + x2 + '.mat'\n",
      "                sio.savemat(path2, {'s_desc': des})\n",
      "    else:\n",
      "        print 'folder ' + str(folder) + ' already exists'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "folder ant already exists\n",
        "folder lotus already exists\n",
        "folder crocodile already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "folder sea_horse already exists\n",
        "folder accordion already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "folder ewer already exists\n",
        "folder buddha already exists\n",
        "folder wild_cat already exists\n",
        "folder lobster already exists\n",
        "folder strawberry already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "folder gramophone already exists\n",
        "folder metronome already exists\n",
        "folder brontosaurus already exists\n",
        "folder kangaroo already exists\n",
        "folder grand_piano already exists\n",
        "folder pigeon already exists\n",
        "folder scissors already exists\n",
        "folder bass already exists\n",
        "folder trilobite already exists\n",
        "folder rhino already exists\n",
        "folder crocodile_head already exists\n",
        "folder wrench already exists\n",
        "folder windsor_chair already exists\n",
        "folder bonsai already exists\n",
        "folder chandelier already exists\n",
        "folder soccer_ball already exists\n",
        "folder octopus already exists\n",
        "folder Faces already exists\n",
        "folder chair already exists\n",
        "folder inline_skate already exists\n",
        "folder wheelchair already exists\n",
        "folder gerenuk already exists\n",
        "folder watch already exists\n",
        "folder dragonfly already exists\n",
        "folder headphone already exists\n",
        "folder dalmatian already exists\n",
        "folder llama already exists\n",
        "folder okapi already exists\n",
        "folder euphonium already exists\n",
        "folder crayfish already exists\n",
        "folder snoopy already exists\n",
        "folder cup already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "folder electric_guitar already exists\n",
        "folder water_lilly already exists\n",
        "folder pagoda already exists\n",
        "folder platypus already exists\n",
        "folder cougar_body already exists\n",
        "folder ibis already exists\n",
        "folder umbrella already exists\n",
        "folder binocular already exists\n",
        "folder dollar_bill already exists\n",
        "folder ferry already exists\n",
        "folder pizza already exists\n",
        "folder dolphin already exists\n",
        "folder menorah already exists\n",
        "folder mandolin already exists\n",
        "folder ketch already exists\n",
        "folder ceiling_fan already exists\n",
        "folder cougar_face already exists\n",
        "folder mayfly already exists\n",
        "folder brain already exists\n",
        "folder schooner already exists\n",
        "folder pyramid already exists\n",
        "folder crab already exists\n",
        "folder Leopards already exists\n",
        "folder BACKGROUND_Google already exists\n",
        "folder camera already exists\n",
        "folder flamingo_head already exists\n",
        "folder flamingo already exists\n",
        "folder Faces_easy already exists\n",
        "folder garfield already exists\n",
        "folder lamp already exists\n",
        "folder airplanes already exists\n",
        "folder emu already exists\n",
        "folder barrel already exists\n",
        "folder joshua_tree already exists\n",
        "folder anchor already exists\n",
        "folder revolver already exists\n",
        "folder Motorbikes already exists\n",
        "folder tick already exists\n",
        "folder rooster already exists\n",
        "folder stegosaurus already exists\n",
        "folder minaret already exists\n",
        "folder helicopter already exists\n",
        "folder butterfly already exists\n",
        "folder hedgehog already exists\n",
        "folder nautilus already exists\n",
        "folder starfish already exists\n",
        "folder sunflower already exists\n",
        "folder panda already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "folder stop_sign already exists\n",
        "folder yin_yang already exists\n",
        "folder hawksbill already exists\n",
        "folder beaver already exists\n",
        "folder saxophone already exists\n",
        "folder cannon already exists\n",
        "folder car_side already exists\n",
        "folder elephant already exists\n",
        "folder stapler already exists\n",
        "folder scorpion already exists\n",
        "folder cellphone already exists\n",
        "folder laptop already exists\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#split data for training and testing\n",
      "list_all_descriptor = []\n",
      "for folder in lstring_folders:\n",
      "    path_folder = newpath + folder + '/'\n",
      "    list_descriptors = os.listdir(path_folder)\n",
      "\n",
      "    # list all files of the class  \n",
      "    files_in_class = []\n",
      "    for descriptor_file in list_descriptors:\n",
      "        path_descriptor_class = path_folder + descriptor_file\n",
      "        files_in_class.append(path_descriptor_class)\n",
      "\n",
      "    #split files of the class\n",
      "    list_all_descriptor.append(files_in_class)\n",
      "\n",
      "#save list in a txt file\n",
      "#thefile = open(path_files_list, 'w')\n",
      "#for classe in files_training:\n",
      "#    for file_mat in classe:\n",
      "#        thefile.write(\"%s\\n\" % file_mat)\n",
      "#thefile.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#print des\n",
      "#print image.split('.',1)[0]\n",
      "#img = cv2.drawKeypoints(y,kp, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
      "#plt.imshow(img, cmap=plt.cm.gray, vmin=0, vmax=255)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Clustering with K-means"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This section compute K-means with a specified k"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "k_clusters = 100          # number of groups to cluster all sift features\n",
      "percentage_test = 0.30    # Test data fraction for the system\n",
      "flag = True\n",
      "\n",
      "path_dataset = './101_ObjectCategories/'\n",
      "lstring_folders = os.listdir(path_dataset)\n",
      "\n",
      "experiment_folder = './experiment_' + str(int(percentage_test*100))+ '_c' + str(k_clusters)\n",
      "path_codebook = experiment_folder + '/code_book.pkl'\n",
      "path_files_train = experiment_folder + '/path_files_train.txt'\n",
      "path_files_test = experiment_folder + '/path_files_test.txt'\n",
      "count_train = 0.0\n",
      "\n",
      "if not os.path.exists(experiment_folder):\n",
      "    os.makedirs(experiment_folder)\n",
      "else:\n",
      "    print 'folder ==> ' + str(experiment_folder) + ' already exists'\n",
      "files_training = []\n",
      "files_test = []"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "folder ==> ./experiment_30_c100 already exists\n"
       ]
      }
     ],
     "prompt_number": 36
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#split data for training and testing\n",
      "for folder in lstring_folders:\n",
      "    if folder != 'BACKGROUND_Google':\n",
      "        path_folder = newpath + folder + '/'\n",
      "        list_descriptors = os.listdir(path_folder)\n",
      "\n",
      "        # list all files of the class  \n",
      "        files_in_class = []\n",
      "        for descriptor_file in list_descriptors:\n",
      "            path_descriptor_class = path_folder + descriptor_file\n",
      "            files_in_class.append(path_descriptor_class)\n",
      "\n",
      "        #split files of the class\n",
      "        a_train, a_test = train_test_split(files_in_class, test_size = percentage_test)\n",
      "        files_training.append(a_train)\n",
      "        files_test.append(a_test)\n",
      "    else:\n",
      "        path_folder = newpath + folder + '/'\n",
      "        list_descriptors = os.listdir(path_folder)\n",
      "\n",
      "        # list all files of the class  \n",
      "        files_in_class = []\n",
      "        for descriptor_file in list_descriptors:\n",
      "            path_descriptor_class = path_folder + descriptor_file\n",
      "            files_in_class.append(path_descriptor_class)\n",
      "        files_test.append(files_in_class)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#save list of files for training and testing\n",
      "if not os.path.isfile(path_files_train):\n",
      "    thefile = open(path_files_train, 'w')\n",
      "    for classe in files_training:\n",
      "        for file_mat in classe:\n",
      "            count_train += 1.0\n",
      "            thefile.write(\"%s\\n\" % file_mat)\n",
      "    thefile.close()\n",
      "\n",
      "    thefile = open(path_files_test, 'w')\n",
      "    for classe in files_test:\n",
      "        for file_mat in classe:\n",
      "            thefile.write(\"%s\\n\" % file_mat)\n",
      "    thefile.close()\n",
      "else:\n",
      "    print 'list of files created'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#compute codebook\n",
      "max_samples = 5000;\n",
      "if not os.path.isfile(path_codebook):\n",
      "    progress = 0.0\n",
      "    index = 0\n",
      "    path_features_to_codebook =  experiment_folder + '/k_groups'\n",
      "    previous_centers = np.empty(shape=[0,128])\n",
      "    list_sift = np.empty(shape=[0,128])\n",
      "\n",
      "    # search for each descriptor of the folder and stack it \n",
      "    for classes in files_training:\n",
      "        temp_sift = np.empty(shape=[0,128])\n",
      "        for descriptor_file in classes:\n",
      "            path_descriptor_load = descriptor_file\n",
      "            #print path_descriptor_load\n",
      "            var = sio.loadmat(path_descriptor_load)\n",
      "            array_to_stack = var['s_desc']\n",
      "            #print array_to_stack\n",
      "            temp_sift = np.vstack([temp_sift, array_to_stack])\n",
      "            progress += 1.0\n",
      "        rows, _ = temp_sift.shape\n",
      "        if max_samples < rows:\n",
      "            definitive_sift = random.sample(temp_sift,max_samples)\n",
      "        else:\n",
      "            definitive_sift = temp_sift\n",
      "        index += 1\n",
      "        list_sift = np.vstack([list_sift, definitive_sift])\n",
      "        print 'stacking...  ' + str(index)+ '  percentage: ' + str((float(progress/count_train))*100) + '%'\n",
      "    k_means = KMeans(init = 'k-means++', n_clusters = k_clusters, n_jobs = -2, n_init=10)\n",
      "    k_means.fit(list_sift)\n",
      "    codebook = k_means\n",
      "    save_object(codebook, path_codebook)\n",
      "    print 'codebook created'\n",
      "else:\n",
      "    print 'Skipping... codebook computation'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Skipping... codebook computation\n"
       ]
      }
     ],
     "prompt_number": 36
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Feature quantization"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This section shows the quantization of features. First we load the codebook calculated from SIFT features and later, we quantize using it"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# initialize parallelism\n",
      "pool = Pool(4)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 51
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#create path for features quantized\n",
      "path_folder_features_train = experiment_folder + '/features_train/'\n",
      "path_folder_features_test = experiment_folder + '/features_test/'\n",
      "\n",
      "# load codebook\n",
      "k_means = load_object(path_codebook)\n",
      "print k_means.cluster_centers_.shape\n",
      "\n",
      "if not os.path.exists(path_folder_features_train):\n",
      "    os.makedirs(path_folder_features_train)\n",
      "\n",
      "    # load data training and quantize features\n",
      "    list_text = get_items_txt(path_files_train)\n",
      "    list_replicate = [path_folder_features_train]*(len(list_text))\n",
      "    list_kmeans = [k_means]*(len(list_text))\n",
      "    list_to_use = zip(list_text, list_replicate, list_kmeans)\n",
      "    pool.map(feature_quantization, list_to_use)\n",
      "else:\n",
      "    print 'folder ==> ' + str(path_folder_features_train) + ' already exists  skipping feature quantization training...'\n",
      "    \n",
      "if not os.path.exists(path_folder_features_test):\n",
      "    os.makedirs(path_folder_features_test)\n",
      "    \n",
      "    # load data testing and quantize features\n",
      "    list_text = get_items_txt(path_files_test)\n",
      "    list_replicate = [path_folder_features_test]*(len(list_text))\n",
      "    list_kmeans = [k_means]*(len(list_text))\n",
      "    list_to_use = zip(list_text, list_replicate, list_kmeans)\n",
      "    pool.map(feature_quantization, list_to_use)    \n",
      "else:\n",
      "    print 'folder ==> ' + str(path_folder_features_test) + ' already exists skipping feature quantization testing...'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(100, 128)\n"
       ]
      }
     ],
     "prompt_number": 60
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "g1 = [1,2,1,2,1]\n",
      "mmm = 'asd'\n",
      "number = len(g1)\n",
      "g2 = [mmm] * number\n",
      "jkl = zip(g1,g2)\n",
      "print len(jkl)\n",
      "print jkl[0][1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "5\n",
        "asd\n"
       ]
      }
     ],
     "prompt_number": 46
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "var = sio.loadmat('./experiment_30_c100/features_train/accordion/feature_image_0001.mat')\n",
      "array_to_stack = var['bow']\n",
      "print array_to_stack"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[ array([[37,  5,  2,  5,  2, 23,  1,  3, 27,  1,  5, 13, 28,  2,  3,  3, 20,\n",
        "         7,  8, 17,  5,  4,  2,  5,  2,  0, 10,  2,  4,  0,  3,  9,  0,  3,\n",
        "         1,  8,  7,  7, 18,  3, 90,  2,  2,  3,  6,  3,  8,  8,  2,  8,  2,\n",
        "        13,  2,  7,  6, 12,  4, 10,  5,  4, 12,  8,  5,  1,  7,  9,  8,  3,\n",
        "         7,  4,  7,  6,  7, 18,  7, 35,  5, 15,  8,  5, 26,  8,  7,  1, 14,\n",
        "         6,  5,  9, 17,  5,  6, 12,  2,  2,  7,  2, 10,  3,  2,  1]])\n",
        "  array([[  0.  ,   0.99,   1.98,   2.97,   3.96,   4.95,   5.94,   6.93,\n",
        "          7.92,   8.91,   9.9 ,  10.89,  11.88,  12.87,  13.86,  14.85,\n",
        "         15.84,  16.83,  17.82,  18.81,  19.8 ,  20.79,  21.78,  22.77,\n",
        "         23.76,  24.75,  25.74,  26.73,  27.72,  28.71,  29.7 ,  30.69,\n",
        "         31.68,  32.67,  33.66,  34.65,  35.64,  36.63,  37.62,  38.61,\n",
        "         39.6 ,  40.59,  41.58,  42.57,  43.56,  44.55,  45.54,  46.53,\n",
        "         47.52,  48.51,  49.5 ,  50.49,  51.48,  52.47,  53.46,  54.45,\n",
        "         55.44,  56.43,  57.42,  58.41,  59.4 ,  60.39,  61.38,  62.37,\n",
        "         63.36,  64.35,  65.34,  66.33,  67.32,  68.31,  69.3 ,  70.29,\n",
        "         71.28,  72.27,  73.26,  74.25,  75.24,  76.23,  77.22,  78.21,\n",
        "         79.2 ,  80.19,  81.18,  82.17,  83.16,  84.15,  85.14,  86.13,\n",
        "         87.12,  88.11,  89.1 ,  90.09,  91.08,  92.07,  93.06,  94.05,\n",
        "         95.04,  96.03,  97.02,  98.01,  99.  ]])]]\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "list_text = get_items_txt(path_files_test)\n",
      "print len(list_text)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "3118\n"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}